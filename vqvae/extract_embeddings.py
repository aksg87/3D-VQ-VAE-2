import pickle
from argparse import ArgumentParser
from pathlib import Path

import lmdb
import numpy as np
import pandas as pd
import torch
from tqdm import tqdm
from utils import CTDataModule

from vqvae.model import VQVAE

GPU = torch.device('cuda')

def extract_samples(model, dataloader):
    model.eval()
    model.to(GPU)

    with torch.no_grad():
        for idx, (sample, _) in enumerate(dataloader):
            sample = sample.to(GPU)
            *_, encoding_idx = zip(*model.encode(sample))
            
            path = dataloader.dataset.dataset.get_path(idx)
            
            yield encoding_idx, path


def get_output_abspath(checkpoint_path: Path, output_path: Path, output_name: str = '') -> str:
    assert output_path.is_dir()
    assert checkpoint_path.is_file()
    checkpoint_path = checkpoint_path.resolve()

    if output_name == '':
        if 'version' in checkpoint_path.parts[-3]:
            print("Assuming checkpoint was generated by slurm")
            output_name = checkpoint_path.parts[-3] + '_' + checkpoint_path.stem 
        else:
            output_name = checkpoint_path.stem
        output_name += '.lmdb'

    return str((output_path / output_name).resolve())


def main(args):
    if args.checkpoint_path is not None:
        model = VQVAE.load_from_checkpoint(str(args.checkpoint_path))
    else:
        model = VQVAE()

    datamodule = CTDataModule(
        path=args.dataset_path,
        batch_size=1,
        train_frac=1,
        num_workers=5,
        rescale_input=(128,128,128)
    )
    datamodule.setup()
    dataloader = datamodule.train_dataloader()

    db = lmdb.open(
        get_output_abspath(args.checkpoint_path, args.output_path, args.output_name),
        map_size=int(1e12),
        # max_dbs=model.n_bottleneck_blocks # REMOVE SUB DATABASES
    )

    # sub_dbs = [db.open_db(str(i).encode()) for i in range(model.n_bottleneck_blocks)]
    # TYPE OF ENCODING
    df = pd.DataFrame(columns=["paths"])
    
    with db.begin(write=True) as txn:
        # Write root db metadata
        txn.put(b"num_dbs", str(model.n_bottleneck_blocks).encode())
        txn.put(b"length",  str(len(dataloader)).encode())
        txn.put(b"num_embeddings", str(model.num_embeddings).encode())
        txn.put(b"n_bottleneck_blocks", str(model.n_bottleneck_blocks).encode())

        for i, (sample_encodings, path) in tqdm(enumerate(extract_samples(model, dataloader)), total=len(dataloader)):

            df.loc[i] = path
            
            sample_encodings_numpy = [sample_encodings[x].cpu().numpy() for x in range(3)]
            txn.put(str(i).encode(), pickle.dumps(sample_encodings_numpy))
                    
            # TODO: Evaluate subdirectory structure
            # for sub_db, encoding in zip(sub_dbs, sample_encodings):
            #     txn.put(str(i).encode(), pickle.dumps(encoding.cpu().numpy()), db=sub_db)

    db.close()
    df.to_csv(args.output_path / "data.csv")

if __name__ == '__main__':
    parser = ArgumentParser()

    parser.add_argument("--output-path", type=Path, default=Path("."))
    parser.add_argument("--output-name", type=str, default='', help="default: takes over the checkpoint name with .lmdb file ext")
    parser.add_argument("--checkpoint-path", type=Path, required=True)
    parser.add_argument("--dataset-path", type=Path, required=True)

    args = parser.parse_args()

    main(args)
